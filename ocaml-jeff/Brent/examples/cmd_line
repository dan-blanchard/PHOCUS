********************************************
Using the command line executable 'segment'

An example to play around with
********************************************

We go through an example with Shakespeare's sonnets.  In the commands below, I
assume that the current working directory is the `examples' folder. I also
assume that the `segment' executable is accessible simply by typing `segment'.
I.e. I assume you have set up an alias to that effect.  If you havent't you
may need to type `.././segment' so that the shell knows to look for the
command in the parent directory and then knows that by virtue of the `./' to
run the executable via ocamlrun.

******
Files
******

The examples folder should contain the following files.

shakespeare_sonnets_gold        : Contains all of Shakespeare's sonnets with their
				  titles commented out.

shakespeare_sonnets_unsegmented : Contains all of Shakespeare's sonnets with
				  their titles commented out, all letters
				  lowercase, spaces and punctuation (except ') 
				  removed.

shakespeare_top_50		: the 50 highest frequency words in these
				  sonnets with their frequencies. This will 
				  be one lexicon we use.

shakespeare_top_50x1000		: The 50 highest frequncy words in these
				  sonnets with their actual frequencies 
				  multiplied by 1000.

**************
Experimenting
**************

You may wonder how the segmenter performs on finding words in Shakespeare's
sonnets, especially given the top 50 most frequently occurring words. I
did. So I typed:

  > segment -l shakespeare_top_50 shakespeare_sonnets_unsegmented > output

On a Pentium M 1.7Ghz intel processor, this took about 15 minutes to complete!
Ther may be ways to improve the code to make it faster but I think I avoided
most of the traps. It takes a long time because the algorithm considers
every possible infix in a string as a word before deciding where to put
boundaries. So if the lines are long, as they are here, it takes a while!

If you do this, you will probably be surprised as I was to see that the
provided lexicon has virtually no effect and the segmenter by the end of the
output assumes each letter is its own word.  Then I tried:

  > segment -l shakespeare_top_50

At the cursor, enter the following followed by either `<eof>' on a line by
itself, or by ^D.

   whatislove
   whatislovethatis
   whatislovethatitis
   whatislovethatitisnot
   whatislovethatitisnotmine
   whatislovethatitisnotminebutthy

(Or copy them into a file and run segment over the file.)  It is striking how
some of the lines are segmented! It is not intuitive to me.

Run it again using the lexicon shakespeare_top_50x1000 and you can see there
is some improvement. 

Generally my experience has been that the segmenter does much better with
shorter utterances than with longer ones.  I suspect that in order to segment
an utterance properly, the frequency of the words in a lexicon grows very fast
wth respect to the length of the utterance.  This would explain why despite
the headstart with lexicon, the segmenter fails to segment the sonnets as we
might hope. 


